{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c333a8-c23d-46e4-98e4-1380f4479b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a company brochure generator with gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657bc5a7-b79c-4b8d-a87c-9e5943c30eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "\n",
    "# DEEPSEEK AZURE OPENAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa64a6e-d7d6-4e0f-b99b-13abedb95c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr # oh yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc5905-eed7-4445-b0bd-ab995aec22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# personalise env data FOR AZURE OPENAI\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URL\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "azure_openai_subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# personalise env data FOR DEEPSEEK AZURE OPENAI\n",
    "azure_deepseek_endpoint = os.getenv(\"DEEPSEEK_AZURE_INFERENCE_SDK_ENDPOINT\")\n",
    "azure_openai_deepseek_endpoint=f\"{azure_deepseek_endpoint}/models\"\n",
    "azure_deepseek_deployment= os.getenv(\"DEPLOYMENT_NAME\", \"DeepSeek-V3-0324\")\n",
    "azure_deepseek_subscription_key = os.getenv(\"DEEPSEEK_AZURE_INFERENCE_SDK_KEY\")\n",
    "\n",
    "# personalise env data FOR GROK AZURE OPENAI\n",
    "azure_grok_endpoint = os.getenv(\"GROK_AZURE_INFERENCE_SDK_ENDPOINT\")\n",
    "azure_grok_deployment = os.getenv(\"GROK_DEPLOYMENT_NAME\", \"grok-3-mini\")\n",
    "azure_grok_subscription_key = os.getenv(\"GROK_AZURE_INFERENCE_SDK_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OPENAI API Key not set\")\n",
    "if azure_openai_subscription_key:\n",
    "    print(f\"AZURE OpenAI API Key exists and begins {azure_openai_subscription_key[:8]}\")\n",
    "else:\n",
    "    print(\"AZURE OPENAI API Key not set\")\n",
    "\n",
    "if azure_deepseek_subscription_key:\n",
    "    print(f\"DEEPSEEK AZURE API Key exists and begins {azure_deepseek_subscription_key[:7]}\")\n",
    "else:\n",
    "    print(\"DEEPSEEK AZURE API Key not set\")\n",
    "\n",
    "if azure_grok_subscription_key:\n",
    "    print(f\"GROK_AZURE API Key exists and begins {azure_grok_subscription_key[:8]}\")\n",
    "else:\n",
    "    print(\"GROK_AZURE API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944c5b4-40ce-4aa7-85cb-b3797c0b7f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google; comment out the Claude or Google lines if you're not using them\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# claude = anthropic.Anthropic()\n",
    "\n",
    "# google.generativeai.configure()\n",
    "\n",
    "# GPT AZURE OPENAIS\n",
    "# Initialize Azure OpenAI client with key-based authentication\n",
    "clientGPTAzure = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_subscription_key,\n",
    "    api_version=\"2025-01-01-preview\",\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI for Deepseek client with key-based authentication\n",
    "# api_version=\"2025-01-01-preview\",\n",
    "clientDeepseekAzureOpenAI = AzureOpenAI(\n",
    "    azure_endpoint=azure_deepseek_endpoint,\n",
    "    api_key=azure_deepseek_subscription_key, \n",
    "    api_version=\"2024-10-21\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# DEEPSEEK AZURE OPENAI\n",
    "clientDeepSeekAzure = ChatCompletionsClient(endpoint=azure_openai_deepseek_endpoint, credential=AzureKeyCredential(azure_deepseek_subscription_key))\n",
    "\n",
    "# GROK AZURE OPENAI\n",
    "clientGrokAzure = ChatCompletionsClient(endpoint=azure_grok_endpoint, credential=AzureKeyCredential(azure_grok_subscription_key))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63b284-0363-4eaa-85ca-196c21d44766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A generic system message - no more snarky adversarial AIs!\n",
    "\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a92952-550f-4d1a-be7f-d9d07d380a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap a call to GPT-4o-mini in a simple function\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ec158-a677-4dcf-a882-67a3b13ae07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Markdown\n",
    "# Are you wondering why it makes any difference to set system_message when it's not referred to in the code below it?\n",
    "# I'm taking advantage of system_message being a global variable, used back in the message_gpt function (go take a look)\n",
    "# Not a great software engineering practice, but quite common during Jupyter Lab R&D!\n",
    "\n",
    "system_message = \"You are a helpful assistant that responds in markdown\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58b28f-2ba4-434e-8c71-535719896ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stream_openai_gpt(prompt):\n",
    "    # stream = openai.chat.completions.create(\n",
    "    #     model='gpt-4o-mini',\n",
    "    #     messages=messages,\n",
    "    #     stream=True\n",
    "    # )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    \n",
    "    stream = clientGPTAzure.chat.completions.create(\n",
    "    model=azure_openai_deployment,\n",
    "    messages=messages,\n",
    "    max_tokens=6553,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None,\n",
    "    stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        if not chunk.choices: \n",
    "            continue\n",
    "        \n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta and getattr(delta, \"content\", None):  \n",
    "            content = delta.content\n",
    "            result += content\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e1745-1b82-425c-9919-59d9baff079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_one = gr.Interface(\n",
    "    fn=stream_openai_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view_one.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d569c1d-00a4-4400-9eb8-de7a3e85d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_deepseek(prompt):\n",
    "    system_message = \"You are a helpful assistant that responds in plain text\"\n",
    "    sysContent=system_message\n",
    "    usrContent=prompt\n",
    "\n",
    "    result = clientDeepSeekAzure.complete(\n",
    "    messages=[\n",
    "    SystemMessage(content=sysContent),\n",
    "    UserMessage(content=usrContent)\n",
    "    ],\n",
    "    model = azure_deepseek_deployment,\n",
    "    max_tokens=1000,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in result:\n",
    "        if not chunk.choices:  # <-- skip empty events\n",
    "            continue\n",
    "        \n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta and getattr(delta, \"content\", None):  # safe check\n",
    "            content = delta.content\n",
    "            response += content\n",
    "            yield response\n",
    "    # print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa71dfd-4c0c-4870-8808-40db1c2ea2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_deepseek,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d946c1-6804-410e-b9fe-2c4e0fc082cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_deepseek_openai(prompt):\n",
    "    system_message = \"You are a helpful assistant that responds in Markdown\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    \n",
    "    stream = clientDeepseekAzureOpenAI.chat.completions.create(\n",
    "    model=azure_deepseek_deployment,\n",
    "    messages=messages,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.8,\n",
    "    top_p=0.1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None,\n",
    "    stream=True\n",
    "    )\n",
    "\n",
    "    # response = clientDeepseekAzureOpenAI.chat.completions.create(\n",
    "    # model=azure_deepseek_deployment, # Replace with your model dpeloyment name.\n",
    "    # messages=[\n",
    "    #     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    #     {\"role\": \"user\", \"content\": \"Explain Riemann's conjecture in 1 paragraph\"}\n",
    "    # ],\n",
    "    # )\n",
    "    # print(response.model_dump_json(indent=2))\n",
    "    # print(stream.model_dump_json(indent=2))\n",
    "    # print(stream.choices[0].model_dump_json(indent=2))\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        if not chunk.choices:  \n",
    "            continue\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta and getattr(delta, \"content\", None): \n",
    "            content = delta.content\n",
    "            result += content\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8129a60-d694-4632-aece-e027c9b9cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_deepseek_openai,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f772ea-c2af-4d3e-a8cb-286702d31c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "class Website:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23c4c8-d66a-4ed8-a308-0ca3ad84fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With massive thanks to Bill G. who noticed that a prior version of this had a bug! Now fixed.\n",
    "\n",
    "system_message = \"You are an assistant that analyzes the contents of a company website landing page \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1e746-0b5e-4521-bc7b-d46a5d5c4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url, model):\n",
    "    yield \"\"\n",
    "    prompt = f\"Please generate a company brochure for {company_name}. Here is their landing page:\\n\"\n",
    "    prompt += Website(url).get_contents()\n",
    "    \n",
    "    if model==\"AZURE_OPENAI_GPT\":\n",
    "        result = stream_openai_gpt(prompt)\n",
    "    elif model==\"OPENAI_DEEPSEEK\":\n",
    "        result = stream_deepseek_openai(prompt)\n",
    "    elif model==\"AZURE_OPENAI_DEEPSEEK\":\n",
    "        result = stream_deepseek(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a08971-5374-4573-a6ab-a36822052f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_brochure,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Company name:\"),\n",
    "        gr.Textbox(label=\"Landing page URL including http:// or https://\"),\n",
    "        gr.Dropdown([\"AZURE_OPENAI_GPT\", \"OPENAI_DEEPSEEK\",\"AZURE_OPENAI_DEEPSEEK\"], label=\"Select model\")],\n",
    "    outputs=[gr.Markdown(label=\"Brochure:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b30cd-ed1a-4eaa-b324-d07d89f651d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
